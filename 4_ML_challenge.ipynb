{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae8ecef5-4cf3-4ec5-9a1f-7eaa6558e488",
   "metadata": {},
   "source": [
    "<p float=\"left\">\n",
    "  <img src=\"climate_net/img/CCfCS.png\" width=\"100\" />\n",
    "  <img src=\"climate_net/img/cam.png\" width=\"100\" />\n",
    "</p>\n",
    "\n",
    "# CCfCS ML for Climate Science -  Notebook 4. Afternoon Session\n",
    "### Designing a machine learning algorithm for predicting extreme weather events in the ClimateNet dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747058a5-81fb-45f7-89d1-42f2754b0c53",
   "metadata": {},
   "source": [
    "The main goal of this afternoon's session is to use the ML tools from the morning session on the data that we initially explored to try and predict whether a given geographical location is experiencing an extreme weather event in the form of an atmospheric river or a tropical cyclone. The first thing to do is to create a training and testing dataset, which were preloaded in the MNIST example.\n",
    "\n",
    "*You must have first added a shortcut to your GDrive - see [here](https://github.com/Ira-Shokar/CCFCS-Machine-Learning-Workshop):*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e4bd25-01ee-4321-b005-e7c6da45d99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "%pip install -r /content/drive/MyDrive/climate_net/env.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a28fd9a-0a12-47a1-a6fa-8620e0231481",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import netCDF4 as nc\n",
    "import matplotlib.pylab as plt\n",
    "import cmocean \n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import urllib.request as urllib2\n",
    "import requests\n",
    "import xarray as xr\n",
    "from scipy import stats\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d217087-c485-4a9f-b44d-828b0f313f03",
   "metadata": {},
   "source": [
    "Load in the data as usual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4536d511-4b95-49ec-99f9-bb7fabb53b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset('/content/drive/MyDrive/climate_net/climatenet_data.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32085293-dfe6-468c-b292-9387f7964ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd59710c-537c-4c13-b51e-50cf3a7f1013",
   "metadata": {},
   "source": [
    "Let us start by choosing some training inputs. You might want to go back to the data exploration notebook for a reminder of what these are, what they look like, and an idea of how they are related to each other. We also define our labels, which our network is trying to predict. Remember, 0 is no extreme event, 1 is an tropical cyclone and 2 is a atmospheric river. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44b4a86-ebb4-4520-81c3-25dd9fc7f57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_1 = ds.U850.data\n",
    "input_2 = ds.V850.data\n",
    "input_3 = ds.TMQ.data\n",
    "input_4 = ds.PSL.data\n",
    "labels = ds.LABELS.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae91911-cbb5-4a8d-96f3-a11dabf7291e",
   "metadata": {},
   "source": [
    "To make the problem more tractable, let's sparse down the data a little by filtering it like we did earlier. Note this might take a little while to run.\n",
    "\n",
    "*Note: is this the best way to filter the data? What happens to the performance of the model if we just sparse regularly, for example? These are questions you might come back to later if you have time. Convolutional networks are a special class of neural network that essentially optimise a filtering process to sequentially reduce dimensionaility whilst extracting relevant spatial information from images.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33ec4ae-7d10-4b36-ba96-15fea4a18c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage.filters import gaussian_filter\n",
    "filter_input_1 = gaussian_filter(input_1,sigma = [0,10,10])[:,::16,::16]\n",
    "filter_input_2 = gaussian_filter(input_2,sigma = [0,10,10])[:,::16,::16]\n",
    "filter_input_3 = gaussian_filter(input_3,sigma = [0,10,10])[:,::16,::16]\n",
    "filter_input_4 = gaussian_filter(input_4,sigma = [0,10,10])[:,::16,::16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faaf7d54-9235-4545-8bd5-20dd641c09a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filter_input_1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6bc0fe-375e-42f6-a98c-e668ad5daeb9",
   "metadata": {},
   "source": [
    "Now we have 48 x 72 = 3456 inputs per time step. We filter the labels slightly differently, i.e. a filtered cell is labelled as an extreme event according to its Gaussian centre cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd3c602-3c06-4213-9c0a-ccf2091bbc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_labels = labels[:,::16,::16]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cfac8b-fb09-4668-8dc8-1ca80ae38448",
   "metadata": {},
   "source": [
    "At this point the data is still in a nice form (i.e. we can do a contour plot for each timestep). Let us choose the first 67 time steps for training, and the remaining 16 for testing. We then need to convert it into the shape (training samples, number of inputs). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6665635-28f6-4204-8756-71e6012d9b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.stack((filter_input_1[:67].flatten(), filter_input_2[:67].flatten(), \n",
    "                         filter_input_3[:67].flatten(), filter_input_4[:67].flatten()), axis=1)\n",
    "X_test = np.stack((filter_input_1[67:].flatten(), filter_input_2[67:].flatten(), \n",
    "                         filter_input_3[67:].flatten(), filter_input_4[67:].flatten()), axis=1)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91838fb5-203b-4a77-b448-b1b7db013646",
   "metadata": {},
   "source": [
    "We need to similarly flatten the labels, and then convert them into the categorical format, just as we did for the MNIST labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e690f12e-5ca3-4f9f-bdd3-b5e3340541b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = filter_labels[:67].flatten()\n",
    "test_labels = filter_labels[67:].flatten()\n",
    "print(train_labels.shape)\n",
    "print(test_labels.shape)\n",
    "nb_classes= 3\n",
    "Y_train = keras.utils.to_categorical(train_labels, num_classes=nb_classes, dtype='int64') \n",
    "Y_test = keras.utils.to_categorical(test_labels, num_classes=nb_classes, dtype='int64') \n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b682c0cd-6856-4c30-b289-b7804fc19787",
   "metadata": {},
   "source": [
    "The final step for preparing the training data is to normalize it. This is particularly important for our dataset since the range of values varies greatly for the different inputs (for example, PSL compared to TMQ). Here, we normalize by subtracting the mean and dividing by the standard deviation. Remember to normalize the test data by the same mean and standard deviation as the training data.\n",
    "\n",
    "*You might want to experiment with different ways of normalizing the data, or look at what happens when there is no normalization.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923c525a-30b1-461f-9d2d-5d65e8007dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = np.mean(X_train, axis=0)\n",
    "sigma = np.std(X_train, axis=0)\n",
    "print(mu, sigma)\n",
    "X_train = (X_train-mu)/sigma #Careful not to run this cell twice \n",
    "X_test = (X_test-mu)/sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797a370f-4e31-4a83-aee3-d23d1dc12a6e",
   "metadata": {},
   "source": [
    "Now that the data is prepared, let's build our first model. This is identical to the MNIST model we built, except that the shape of the inputs and outputs has been changed to match our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf37d99-1569-4698-8765-046bda136dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[4, 1]),\n",
    "    tf.keras.layers.Dense(200, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(60, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(3, activation='sigmoid')]) # classifying into 3 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ddc908-97fd-4436-a7c1-d3af7f14306b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c33320-c783-43b3-8052-80d91542540f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss= 'categorical_crossentropy',\n",
    "    metrics=['accuracy']) # % of correct answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bb5bd7-f875-4237-beb2-0d7eef1b6325",
   "metadata": {},
   "source": [
    "Train the model, just as we did for the MNIST data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa2551d-cf7e-4f4e-8155-b8ef624e633a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, Y_train, batch_size=128, epochs=5, validation_split = 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1726aad3-d720-4018-9b97-a4e51d7f6ca1",
   "metadata": {},
   "source": [
    "We've trained our first model for making predictions on the data. Now let's have a look at how the model performs on the test set. We'll start by doing things qualitatively: reshaping the predicted classes back into their original spatio-temporal form and then plotting some time slices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff12b66d-e53e-4075-89f3-c69e2d41aa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_classes = model.predict(X_test)\n",
    "predicted_labels = np.argmax(predicted_classes, axis=1).reshape((16, 48, 72))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7605fe-1834-41b0-9037-73ffc6828db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "for timestep in range(5):\n",
    "    plt.subplot(5,2,2*timestep+1)\n",
    "    plt.pcolormesh(predicted_labels[timestep])\n",
    "    if timestep==0:\n",
    "        plt.title('Predictions')\n",
    "    plt.subplot(5,2,2*timestep+2)\n",
    "    plt.pcolormesh(np.reshape(test_labels, (16,48,72))[timestep])\n",
    "    if timestep==0:\n",
    "        plt.title('True labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d5eae7-a188-40d4-9d51-f21bd64b2dd1",
   "metadata": {},
   "source": [
    "At least qualititavely, our models seems to be doing an OK job at predicting atmospheric rivers (in yellow) and a poor job at predicting tropical cyclones (in turquoise). Let's define and discuss some quantitative measures to expand on this. Firstly, accuracy: how often do predictions=labels?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d128d440-e54a-4dbb-90fd-9d40ad53d264",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predicted, true):\n",
    "    return np.sum(predicted==true)/predicted.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f4a565-af1a-49fc-9795-f5497c1d4486",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy(predicted_labels.flatten(), test_labels.flatten()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54b5367-0f5c-42ba-b4ba-e5a9adb5ab9e",
   "metadata": {},
   "source": [
    "The model has very good accuracy on the test set. But how meaningful is this? Consider a baseline model that predicts class 0 for every grid cell. Then since most of the grid cells are indeed class 0, the accuracy will naturally be very high. Indeed, we can compute it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bbf7e0-f41d-4127-9e1a-37c316108f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy(np.zeros(test_labels.shape[0]), test_labels.flatten()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5956ce08-7d44-4b74-959b-adc19761a179",
   "metadata": {},
   "source": [
    "Actually, our model only just exceeds the baseline by this metric. Ideally we would like a metric that captures how often the model is right when it predicts an extreme weather event. If we define a 'positive' to be a given class then we can define **precision** and **recall** as:\n",
    "\n",
    "Precision = #True positives/(#True positives + #False positives) \n",
    "\n",
    "Recall = #True positives/(#True positives + #False negatives)\n",
    "\n",
    "Here a true positive means the model correctly predicted true, and a false positive means the model incorrectly predicted true. If we want to seek a balance between precision and recall we can use the so-called F1 score:\n",
    "\n",
    "F1 = 2 x (precision x recall)/(precision + recall)\n",
    "\n",
    "For an extended discussion on the meaning of these metrics, see this article: https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbef40cb-0c14-4e32-a046-6b6c83a63433",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_label_n(predicted, true, label_n):\n",
    "    tp = ((predicted==label_n).astype(int)*(true==label_n).astype(int)).sum() #number of true positives\n",
    "    fp = ((predicted==label_n).astype(int)*(1-(true==label_n).astype(int))).sum() #number of false positives\n",
    "    return tp/(tp+fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e31e34-a270-441c-8fa5-98d29f2ce7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Cyclone precision', precision_label_n(predicted_labels.flatten(), test_labels.flatten(), label_n=1))\n",
    "print('AR precision', precision_label_n(predicted_labels.flatten(), test_labels.flatten(), label_n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76659706-0edb-4fa8-ae37-07a51cf8576d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_label_n(predicted, true, label_n):\n",
    "    tp = ((predicted==label_n).astype(int)*(true==label_n).astype(int)).sum() #number of true positives\n",
    "    fn = ((1-(predicted==label_n).astype(int))*((true==label_n).astype(int))).sum() #number of false negatives\n",
    "    return tp/(tp+fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1b3af3-d474-4391-88d7-fa7d61edc61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Cyclone recall', recall_label_n(predicted_labels.flatten(), test_labels.flatten(), label_n=1))\n",
    "print('Atmospheric river recall', recall_label_n(predicted_labels.flatten(), test_labels.flatten(), label_n=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15203e77-f8bf-415b-a209-e3c7b878de3c",
   "metadata": {},
   "source": [
    "There is actually a pre-existing library function that will do this for you, for each label. The 'support' is just the number of (true) labels from a given class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa140766-a842-4ea4-b6a2-2b618213fbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "precision, recall, fscore, support = score(test_labels.flatten(), predicted_labels.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d257a2d3-d945-4b75-8c67-551c4d377dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('precision: {}'.format(precision))\n",
    "print('recall: {}'.format(recall))\n",
    "print('fscore: {}'.format(fscore))\n",
    "print('support: {}'.format(support))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c829f12e-a090-4a05-adce-516e8f0beb37",
   "metadata": {},
   "source": [
    "Here we can clearly see that the fact that the number of instances of each class is seriously skewed is causing some problems for the model. In particular, the recall scores are particularly poor for extreme weather events, i.e. there are a lot of false negatives. This is bad news: we would probably rather have an overcautious model rather than our current very undercautious one (i.e. fails to predict a lot of true extreme weather events). \n",
    "\n",
    "Of course, we should keep in mind that evaluating our model cell-by-cell is also probably not the best measure of performance: certainly the qualitative picture seems to suggest the regional performance on atmospheric rivers is actually OK: i.e. it gets the locations roughly correct (which cell-by-cell metrics of performance do not capture completely)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6438b41-9a05-4f28-bcf3-949abfe8c330",
   "metadata": {},
   "source": [
    "# The task\n",
    "We have built a very basic model for predicting atmospheric weather events, and whilst it isn't great, there is an indication that it is picking up on some trends. By copying and adapting the code above, can you improve the model? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee0c882-ac2d-426f-a928-eb71ed212bbe",
   "metadata": {},
   "source": [
    "Some ideas to get you started:\n",
    "\n",
    "1. You could try getting started by simply training the model for more epochs. How much does this improve the performance metrics?\n",
    "2. Next, you might try changing the model design. Why not play around with the number of layers, the number of neurons per layer, and the activation functions?\n",
    "3. You might try changing the inputs to the model (look at the data loading notebook from this morning for some ideas). You could also try adding more inputs to the model, changing the filtering procedure, and changing the normalization.\n",
    "4. **Extensions**: there is certainly spatial information in the original data that will be useful to a model for detecting extreme weather events. How might you harness this? One simple idea is to actually have latitude and/or longitude values of each grid cell each *as inputs to the model*. Another simple idea is to reduce the size of the dataset by filtering out the high latitudes. This will reduce the skew in the number of labels for each class. A more complex idea is to create a dataset which predicts a cell label based on data values from that cell and the adjacent cells (note this will require substantially reshaping your training data). \n",
    "\n",
    "Do feel free to discuss with each other and Sam/Seb/Ira if you aren't sure what to do! We do stress that the aim of this workshop is not to create a ground-breaking model for predicting extreme weather events, but rather to get experience constructing a dataset, and playing around with simple machine learning models for making predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ec0a2b-14d8-4dd6-bf77-d70b61358e77",
   "metadata": {},
   "source": [
    "### Choose inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9319da44-1aa2-4bf0-a6c1-48b12874fadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_1 = ds.U850.data\n",
    "input_2 = ds.V850.data\n",
    "input_3 = ds.TMQ.data\n",
    "input_4 = ds.PSL.data\n",
    "# input_5 = ? \n",
    "labels = ds.LABELS.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc09592f-996a-480d-976b-6a9010bec79b",
   "metadata": {},
   "source": [
    "### Filter to reduce number of data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fead7a-fd1b-440a-8545-deba0e0c5975",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage.filters import gaussian_filter\n",
    "filter_input_1 = gaussian_filter(input_1,sigma = [0,10,10])[:,::16,::16]\n",
    "filter_input_2 = gaussian_filter(input_2,sigma = [0,10,10])[:,::16,::16]\n",
    "filter_input_3 = gaussian_filter(input_3,sigma = [0,10,10])[:,::16,::16]\n",
    "filter_input_4 = gaussian_filter(input_4,sigma = [0,10,10])[:,::16,::16]\n",
    "filter_labels = labels[:,::16,::16]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f914a20e-c9bd-4a02-80bf-e7c2b39e774f",
   "metadata": {},
   "source": [
    "### Build training and testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f010e9-99e5-4fea-9c75-7ebef0cecf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.stack((filter_input_1[:67].flatten(), filter_input_2[:67].flatten(), \n",
    "                         filter_input_3[:67].flatten(), filter_input_4[:67].flatten()), axis=1)\n",
    "X_test = np.stack((filter_input_1[67:].flatten(), filter_input_2[67:].flatten(), \n",
    "                         filter_input_3[67:].flatten(), filter_input_4[67:].flatten()), axis=1)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb78a287-76c5-42d6-860c-b60df7b89575",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = filter_labels[:67].flatten()\n",
    "test_labels = filter_labels[67:].flatten()\n",
    "print(train_labels.shape)\n",
    "print(test_labels.shape)\n",
    "nb_classes= 3\n",
    "Y_train = keras.utils.to_categorical(train_labels, num_classes=nb_classes, dtype='int64') \n",
    "Y_test = keras.utils.to_categorical(test_labels, num_classes=nb_classes, dtype='int64') \n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03442a4-0560-4db1-bc6f-b6d93e21c9e3",
   "metadata": {},
   "source": [
    "### Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29eb2d8-8ecc-4b30-a0bd-3e27f4ee835a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = np.mean(X_train, axis=0)\n",
    "sigma = np.std(X_train, axis=0)\n",
    "print(mu, sigma)\n",
    "X_train = (X_train-mu)/sigma #Careful not to run this cell twice \n",
    "X_test = (X_test-mu)/sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a977c0-7b9e-42b7-a0df-82f993594675",
   "metadata": {},
   "source": [
    "### Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893b8710-bd00-48c2-add7-ed70ca11ecb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[4, 1]),\n",
    "    tf.keras.layers.Dense(200, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(60, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(3, activation='sigmoid')]) # classifying into 3 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d500359a-440b-4bc6-a098-226118a2a7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bec5ac0-b46c-4256-b67e-17c31c640ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss= 'categorical_crossentropy',\n",
    "    metrics=['accuracy']) # % of correct answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b6c1b5-0da5-4c3f-901f-d197014c6e75",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53dccde-4f77-431b-9151-911630bcb5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, Y_train, batch_size=128, epochs=10, validation_split = 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3dde070-d0b3-4fb5-b27f-4cc7fe9b51ac",
   "metadata": {},
   "source": [
    "### Test model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a0a8ff-a0b3-47a4-9856-0421dbcafab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_classes = model.predict(X_test)\n",
    "predicted_labels = np.argmax(predicted_classes, axis=1).reshape((16, 48, 72))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339a828e-1e3f-4f31-ae8f-de73bcd4f055",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "for timestep in range(5):\n",
    "    plt.subplot(5,2,2*timestep+1)\n",
    "    plt.pcolormesh(predicted_labels[timestep])\n",
    "    if timestep==0:\n",
    "        plt.title('Predictions')\n",
    "    plt.subplot(5,2,2*timestep+2)\n",
    "    plt.pcolormesh(np.reshape(test_labels, (16,48,72))[timestep])\n",
    "    if timestep==0:\n",
    "        plt.title('True labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd523249-4c89-4f6b-9f29-5be0e6536871",
   "metadata": {},
   "source": [
    "### Performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7310800a-dab0-4d96-a2d6-ef2208451db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "precision, recall, fscore, support = score(test_labels.flatten(), predicted_labels.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc0a798-1a77-4472-9434-2f8b89643e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('precision: {}'.format(precision))\n",
    "print('recall: {}'.format(recall))\n",
    "print('fscore: {}'.format(fscore))\n",
    "print('support: {}'.format(support))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

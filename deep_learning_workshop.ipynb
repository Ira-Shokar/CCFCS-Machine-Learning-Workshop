{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, matplotlib.pyplot as plt, tensorflow as tf, tensorflow.keras as keras # Package imports "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks - Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspired by the brain, the neurons refer to objects that hold information, and they are connected by functions that form the network structure.\n",
    "\n",
    "As with the other machine learning methods mentioned earlier, a model must be trained on data, before the trained model can be used for inference on new data. Again, the data can come in both the form of labelled and unlabelled data for various supervised and unsupervised tasks.\n",
    "\n",
    "To explore Deep Learning we will use the task of image classification. Here we will use a deep neural network to classify handwritten digits from the MNIST database which contains 60,000 images for training and 10,000 test images. First we will load our dataset.\n",
    "\n",
    "#### Load MNIST Handwritten Digits Dataset\n",
    "\n",
    "Here the inputs to the network are the N x N pixel images, with N=28, and the output layer consists of 10 neurons, each reprenting a value in the set of [0, 9]. Here a trained network should be able to take in an input image, process what within the image characterises the digit that is represents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import MNIST dataset\n",
    "# The MNIST data is split between 60,000 28 x 28 pixel training images and 10,000 28 x 28 pixel images\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "print(\"X_train shape\", X_train.shape)\n",
    "print(\"y_train shape\", y_train.shape)\n",
    "print(\"X_test shape\", X_test.shape)\n",
    "print(\"y_test shape\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualise subset of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,9))\n",
    "\n",
    "for i in range(9):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    num = np.random.randint(0, len(X_train))\n",
    "    plt.imshow(X_train[num], cmap='gray', interpolation='none')\n",
    "    plt.title(\"Class {}\".format(y_train[num]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Format Data for Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32')/255  # Normalising data and setting data type to float\n",
    "X_test = X_test.astype('float32')/255    # Normalising data and setting data type to float\n",
    "\n",
    "nb_classes = 10 # number of classes, representing the number of unquie digits\n",
    "\n",
    "Y_train = keras.utils.to_categorical(y_train, nb_classes) \n",
    "Y_test = keras.utils.to_categorical(y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would a neural network do this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed Forward Neural Netwok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic network strucures have an input layer, of size corresponding to the \"dimension\" of the incoming data, and an outplut layer that is specific to the task. For a regression task this may be a single neuron, to give a single value, or for a classification task this would correspond to the number of classes.\n",
    "\n",
    "What makes Neural Networks powerful, over the methods used earlier is their ability to have hidden layer between the input and output layers, and this ability to nest operations makes them able to capture more complex patterns in the data - these additional layers are referred to as 'Hidden Layers'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![FFNN](img/nn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This image shows a feed forward neural network- named as such because nothing particualrly special is taking place, the outputs of one layer, become the inputs to the next layer, and thus the information is few forward through the network from the input layer to the output layer.\n",
    "\n",
    "This strucure is supposed to be loosely based on biological structures, such as the brain, where information from one neuron firing causes other neurons to fire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned this is a Feed Forward Neural Network, however there are other network architectures that are used for various tasks, some basic examples are:\n",
    "\n",
    "Other Types of Network Architectures:\n",
    "    - Convolutional Neural Network - Image Processing\n",
    "    - Recurrent Neural Networks - Time Series Data\n",
    "    - Graph Neural Network - \n",
    "\n",
    "However, there are variations of each of these, and developing new archictures is a very active research field, where the balance between model acuracy and computataional expense are being ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature recognition\n",
    "\n",
    "Now how would the network learn these features, before jumping into the mathematics of this process, lets think of how a human would learn identifiy a '9' for example. One may first identify the loop at the top, and secondarily the the line that comes down from that loop. Of course when also shown another image of a 9, a human can still indentify that the image is indeed of a 9, eventhough the image is not quite the same as the fist, where the loop is different and the line is now curved. When training out network, we hope that it too would be invarient to these small differences and would not classify based on exact locations of pixels within the image domian, but use structres, or features, within the image that are learnt to corespond to the image label.\n",
    "\n",
    "This is where the notion of layers comes in ... not specific to images here, this could be a time series corresponding to speech, where raw audo must be broken down into district sounds that together form syllables which in turn form words... breaks down into layers of abstraction ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28, 1]),\n",
    "    tf.keras.layers.Dense(200, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(60, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')]) # classifying into 10 classes\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### parameters - weights and biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relu, softmax and sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### maths - matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does it 'learn'?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cost function\n",
    "\n",
    "Cross entropy loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### how do we find minima - high dimensional space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient decent (stochastic?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### local minima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step size - learning \n",
    "\n",
    "\"Learning rate\": You cannot update your weights and biases by the whole length of the gradient at each iteration. It would be like trying to get to the bottom of a valley while wearing seven-league boots. You would be jumping from one side of the valley to the other. To get to the bottom, you need to do smaller steps, i.e. use only a fraction of the gradient, typically in the 1/1000th range. This fraction is called the \"learning rate\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update parameters - backpropogation\n",
    "\n",
    "- Too expensive to conduct this for each training example for each gradient decent step.\n",
    "- shuffle and divide into mini batches - compute step according to the mini batch, this also better represents the constraints imposed by different example images and is therefore likely to converge towards the solution faster.\n",
    "- The size of this mini batch is an adjustable parameter.\n",
    "- This technique, sometimes called \"stochastic gradient descent\" has another, more pragmatic benefit: working with batches also means working with larger matrices and these are usually easier to optimise on GPUs and TPUs.\n",
    "\n",
    "-momentum\n",
    "\n",
    "-explodiing and vanishing gradients\n",
    "\n",
    "The cross-entropy formula involves a logarithm and log(0) is Not a Number (NaN, a numerical crash if you prefer). Can the input to the cross-entropy be 0? The input comes from softmax which is essentially an exponential and an exponential is never zero. So we are safe!\n",
    "\n",
    "Really? In the beautiful world of mathematics, we would be safe, but in the computer world, exp(-150), represented in float32 format, is as ZERO as it gets and the cross-entropy crashes.\n",
    "\n",
    "Fortunately, there is nothing for you to do here either, since Keras takes care of this and computes softmax followed by the cross-entropy in an especially careful way to ensure numerical stability and avoid the dreaded NaNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuring the model is done in Keras using the model.compile function. Here we use the basic optimizer 'sgd' (Stochastic Gradient Descent). A classification model requires a cross-entropy loss function, called 'categorical_crossentropy' in Keras. Finally, we ask the model to compute the 'accuracy' metric, which is the percentage of correctly classified images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this configures the training of the model. Keras calls it \"compiling\" the model.\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss= 'categorical_crossentropy',\n",
    "    metrics=['accuracy']) # % of correct answers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, Y_train,\n",
    "          batch_size=128, epochs=5,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " and with that information, will output an array of values analogously correspond to the network's confidence in.\n",
    "\n",
    " Here the output will be that array of 10 values, that correspond 'anaolgously' to the network's confidence in classifying the image as that value. I.e., as can be seen here, the network, attibutes the largest value to the number 'N' - which can be interpreteed as the network 'guessing' that this image is a number 'N', while, with a small probability, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, Y_test)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The predict_classes function outputs the highest probability class according to the trained classifier for each input example.\n",
    "predicted_classes = model.predict(X_test)\n",
    "\n",
    "# Check which items we got right / wrong\n",
    "correct_indices, incorrect_indices = [], []\n",
    "for index, prediction_array in enumerate(predicted_classes):\n",
    "    prediction = np.argmax(prediction_array)  #use the largest value of the array as the network's prediction\n",
    "    Y_label    = np.argmax(Y_test[index])     #use the largest value of the array as the truth label\n",
    "    if prediction == Y_label:   correct_indices.append(index)\n",
    "    else:                     incorrect_indices.append(index)\n",
    "\n",
    "plt.figure(figsize=(9,9))   \n",
    "for index, correct in enumerate(correct_indices[:9]):\n",
    "    plt.subplot(3,3,index+1)\n",
    "    plt.imshow(X_test[correct].reshape(28,28), cmap='gray', interpolation='none')\n",
    "    plt.title(\"Predicted {}, Class {}\".format(np.argmax(predicted_classes[correct]), y_test[correct]))\n",
    "plt.show()\n",
    "    \n",
    "plt.figure(figsize=(9,9))\n",
    "for index, incorrect in enumerate(incorrect_indices[:9]):\n",
    "    plt.subplot(3,3,index+1)\n",
    "    plt.imshow(X_test[incorrect].reshape(28,28), cmap='gray', interpolation='none')\n",
    "    plt.title(\"Predicted {}, Class {}\".format(np.argmax(predicted_classes[incorrect]), y_test[incorrect]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Art vs Science - Experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does increasing the batch size to 10,000 affect the training time and test accuracy?\n",
    "\n",
    "How about a batch size of 32?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning rate decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conv Nets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "17b820bf751b809b21a5b68d4073505dbcbcd2e1776d288dde8ec444487d8b66"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('is500')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
